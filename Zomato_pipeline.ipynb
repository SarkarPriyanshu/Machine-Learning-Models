{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFL8mvggztRmrbmz+epwVQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SarkarPriyanshu/Machine-Learning-Models/blob/main/Zomato_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install feature_engine"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VY_lBjJjWyH",
        "outputId": "42e18298-e26f-4a8f-9838-cec48fafa941"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting feature_engine\n",
            "  Downloading feature_engine-1.5.2-py2.py3-none-any.whl (290 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.0/290.0 KB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: statsmodels>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from feature_engine) (0.12.2)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from feature_engine) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.18.2 in /usr/local/lib/python3.8/dist-packages (from feature_engine) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from feature_engine) (1.0.2)\n",
            "Requirement already satisfied: pandas>=1.0.3 in /usr/local/lib/python3.8/dist-packages (from feature_engine) (1.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.0.3->feature_engine) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.0.3->feature_engine) (2022.7.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=1.0.0->feature_engine) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=1.0.0->feature_engine) (3.1.0)\n",
            "Requirement already satisfied: patsy>=0.5 in /usr/local/lib/python3.8/dist-packages (from statsmodels>=0.11.1->feature_engine) (0.5.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from patsy>=0.5->statsmodels>=0.11.1->feature_engine) (1.15.0)\n",
            "Installing collected packages: feature_engine\n",
            "Successfully installed feature_engine-1.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "56-lh5eoY19C"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import scipy.stats as stats\n",
        " \n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn.pipeline import Pipeline\n",
        "from feature_engine.imputation import MeanMedianImputer,AddMissingIndicator,CategoricalImputer\n",
        "from feature_engine.transformation import LogTransformer\n",
        "from feature_engine.encoding import OrdinalEncoder,OneHotEncoder\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "# to build the models\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/gdrive/', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-Vp-ef-Y8fM",
        "outputId": "a9ad603e-e4d3-41b1-bd5a-93b68dc46be0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class handleMixLabels(BaseEstimator, TransformerMixin):\n",
        "  def __init__(self,variables,target,limit=10):\n",
        "    self.variables = variables\n",
        "    self.keys = list()\n",
        "    self.values = list()\n",
        "    self.limit = limit\n",
        "    self.target = target\n",
        "    self.all_unique_feature_type = list()\n",
        "    self.top_unique_feature_type_ = list()\n",
        "    self.unique_rare_feature_type_ = list()\n",
        "    pass\n",
        "  \n",
        "  def fit(self, X: pd.DataFrame,y:pd.Series):\n",
        "        X = X.dropna().copy()\n",
        "        X[self.target] = y\n",
        "        X[self.target] = X[self.target].fillna(X[self.target].median())\n",
        "       \n",
        "        if X[self.target].dtypes != float:\n",
        "          X[self.target] = X[self.target].str.replace(',','').astype(float)\n",
        "        \n",
        "        for key,value in X.groupby(self.variables)[self.target].mean().sort_values(ascending=False).items():\n",
        "                  self.keys.append(key)\n",
        "                  self.values.append(value)\n",
        "\n",
        "        for value in X[self.variables]:\n",
        "          if ',' in value:\n",
        "            for item in value.split(','):\n",
        "              if item.strip() not in self.all_unique_feature_type:\n",
        "                self.all_unique_feature_type.append(item.strip())\n",
        "          else:\n",
        "            if value.strip() not in self.all_unique_feature_type:\n",
        "              self.all_unique_feature_type.append(value.strip())  \n",
        "        \n",
        "        for value in self.keys[:self.limit]:\n",
        "          if isinstance(value,tuple):\n",
        "            for item in value:\n",
        "              if item.strip() not in self.top_unique_feature_type_:\n",
        "                  self.top_unique_feature_type_.append(item.strip())\n",
        "          elif ',' in value:\n",
        "              for item in value.split(','):\n",
        "                if item.strip() not in self.top_unique_feature_type_:\n",
        "                  self.top_unique_feature_type_.append(item.strip())\n",
        "          else:\n",
        "            if value.strip() not in self.top_unique_feature_type_:\n",
        "              self.top_unique_feature_type_.append(value.strip())\n",
        "\n",
        "        self.top_unique_feature_type_ = self.top_unique_feature_type_ + ['Rare']\n",
        "        self.unique_rare_feature_type_ = [value for value in self.all_unique_feature_type if value not in self.top_unique_feature_type_]\n",
        "\n",
        "        return self\n",
        "\n",
        "  def transform(self, X: pd.DataFrame):\n",
        "        X = X.copy()\n",
        "        \n",
        "        # adding new columns of unique labels in datasets\n",
        "        for value in self.top_unique_feature_type_:\n",
        "          X[f'{self.variables}_{value}'] = np.zeros(X.shape[0])\n",
        "\n",
        "        # Adding 1 and 0's to those newly added columns\n",
        "        for value in self.top_unique_feature_type_:\n",
        "          for index in range(0,X.shape[0]):\n",
        "            if  ',' in  X[self.variables][index]:\n",
        "               for item in X[self.variables][index].split(','):\n",
        "                 if item not in self.unique_rare_feature_type_:\n",
        "                    X[f'{self.variables}_{value}'][index] = 1\n",
        "                 if item.strip() in self.unique_rare_feature_type_:\n",
        "                   X[f'{self.variables}_Rare'][index] = 1\n",
        "            else:\n",
        "              if value == X[self.variables][index]:\n",
        "                    X[f'{self.variables}_{value}'][index] = 1\n",
        "              if X[self.variables][index] in self.unique_rare_feature_type_:\n",
        "                   X[f'{self.variables}_Rare'][index] = 1      \n",
        "\n",
        "        return X"
      ],
      "metadata": {
        "id": "IwIE9MNidizS"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ZomatoModelTrain(handleMixLabels):\n",
        "\n",
        "  def __init__(self,df):\n",
        "    self.__df = df\n",
        "    self.__target = 'approx_cost(for two people)'\n",
        "    self.__df[self.__target] = self.__df[self.__target].str.replace(',','').astype(float)\n",
        "    self.__random_state = 100\n",
        "    self.__alpha = 0.001\n",
        "    self.__test_size = 0.33\n",
        "    self.__handleMixLabels = handleMixLabels\n",
        "    self.__variable_to_drop = ['url','address','phone','reviews_list','name','dish_liked','menu_item'] + [self.__target]\n",
        "    self.__AddMissingIndicatorVariables = ['votes','rate','location']\n",
        "    self.__MeanMedianImputerVarables = ['votes','location']\n",
        "    self.__CategoricalImputerModeVarables = ['rate','cuisines','rest_type']\n",
        "    self.__LogTransformerVarables = ['votes']\n",
        "    self.__OrdinalEncoderVariables = ['rate','location','listed_in(type)','listed_in(city)']\n",
        "    self.__OneHotEncoderVariables = ['online_order','book_table','rest_type','cuisines']\n",
        "\n",
        "  def applyModelTrain(self):\n",
        "    X_train,X_test,y_train,y_test = self.__dataSpliter()\n",
        "\n",
        "    X_train,X_test = self.__dataCleanar(X_train,X_test)  \n",
        "    \n",
        "    return X_train,X_test,y_train.fillna(y_train.median()),y_test.fillna(y_test.median())\n",
        "\n",
        "  # Splits the data into train and test set\n",
        "  def __dataSpliter(self):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(df.drop(self.__variable_to_drop,axis=1), df[self.__target], test_size=self.__test_size, random_state=42)\n",
        "    X_train,X_test,y_train,y_test = X_train.reset_index().drop('index',axis=1),X_test.reset_index().drop('index',axis=1),y_train.reset_index().drop('index',axis=1),y_test.reset_index().drop('index',axis=1)\n",
        "    X_train,X_test = self.__dataCleanar(X_train,X_test)\n",
        "    return X_train,X_test,y_train,y_test\n",
        "\n",
        "  # This method andle noisy data from rate and votes columns\n",
        "  def __dataCleanar(self,X_train,X_test):\n",
        "    # replacing '-' with nan in rate variable\n",
        "    X_train['rate'] = X_train['rate'].replace('-',np.nan)  \n",
        "    X_test['rate'] = X_test['rate'].replace('-',np.nan)\n",
        "\n",
        "    # replacing '0' with nan in votes variable\n",
        "    X_train['votes'] = X_train['votes'].replace(0,np.nan)\n",
        "    X_test['votes'] = X_test['votes'].replace(0,np.nan)\n",
        "    \n",
        "    # replacing '/5' with ' in rates variable\n",
        "    X_train['rate'] = X_train['rate'].apply(lambda value:str(value).replace('/5',''))\n",
        "    X_test['rate'] = X_test['rate'].apply(lambda value:str(value).replace('/5',''))\n",
        "    \n",
        "    X_train['rate'] = X_train['rate'].apply(lambda value: 0 if value == 'NEW' else value).astype(float)\n",
        "    X_test['rate'] = X_test['rate'].apply(lambda value: 0 if value == 'NEW' else value).astype(float)\n",
        "\n",
        "    return X_train,X_test\n",
        "\n",
        "  # This method is for rate column where based on certain ranges we made this column to categorical oridinal variable\n",
        "  def handleRating(self,X_train,X_test,feature):\n",
        "    return np.where(X_train[feature]==np.nan,np.nan,np.where(X_train[feature]==0,'New',np.where(X_train[feature]<2.5,'Poor',np.where((X_train[feature]>2.5) | (X_train[feature]<3.5),'Average','Good')))),np.where(X_test[feature]==np.nan,np.nan,np.where(X_test[feature]==0,'New',np.where(X_test[feature]<2.5,'Poor',np.where((X_test[feature]>2.5) | (X_test[feature]<3.5),'Average','Good')))) \n",
        "\n",
        "\n",
        "  # This method checks relation with target columns based on that montonic relation assign ordinal values to top 10 features and map those feature on train and test datasets \n",
        "  def handleRanking(self,X_train,X_test,feature,tol=10):\n",
        "    if len(df[feature].unique()) > 15:\n",
        "      # Get unique top 10 appering categories\n",
        "      listed_in_ranks = list(self.__df.groupby(feature)[self.__target].mean().sort_values(ascending=False)[:tol].to_dict().keys())\n",
        "\n",
        "      # replacing non top categories  \n",
        "      X_train[feature] = X_train[feature].apply(lambda value:value if value in listed_in_ranks else 'Rare')\n",
        "      X_test[feature] = X_test[feature].apply(lambda value:value if value in listed_in_ranks else 'Rare')   \n",
        "      \n",
        "      listed_in_ranks = listed_in_ranks + ['Rare']\n",
        "\n",
        "      # Creating dictionary for mapping categories \n",
        "      listed_in_dict = dict()\n",
        "      for index in range(0,len(listed_in_ranks)):\n",
        "        listed_in_dict[listed_in_ranks[index]] = index\n",
        "\n",
        "      # replacing categories\n",
        "      X_train[feature] = X_train[feature].map(listed_in_dict)\n",
        "      X_test[feature] = X_test[feature].map(listed_in_dict)\n",
        "\n",
        "    else:\n",
        "      # Get unique top 10 appering categories\n",
        "      listed_in_ranks = list(self.__df.groupby(feature)[self.__target].mean().sort_values(ascending=False)[:tol].to_dict().keys())\n",
        "      \n",
        "      # Creating dictionary for mapping categories)\n",
        "      listed_in_dict = dict()\n",
        "      for index in range(0,len(listed_in_ranks)):\n",
        "        listed_in_dict[listed_in_ranks[index]] = index\n",
        "\n",
        "      # replacing categories\n",
        "      X_train[feature] = X_train[feature].map(listed_in_dict)\n",
        "      X_test[feature] = X_test[feature].map(listed_in_dict)  \n",
        "\n",
        "    return X_train,X_test     \n",
        "\n",
        "\n",
        "  def featurePipeline(self):\n",
        "\n",
        "    pipe = Pipeline([\n",
        "\n",
        "      #   Missing indicator\n",
        "      ('Add missing indicator',AddMissingIndicator(\n",
        "          variables=self.__AddMissingIndicatorVariables)),\n",
        "    \n",
        "      #   Median Missing Imputation\n",
        "      ('Median Missing Imputation',MeanMedianImputer(\n",
        "          imputation_method='median', variables=self.__MeanMedianImputerVarables)),\n",
        "\n",
        "      #   Mode Missing Imputation\n",
        "      ('Mode Missing Imputation',CategoricalImputer(\n",
        "          imputation_method='frequent', variables=self.__CategoricalImputerModeVarables)),\n",
        "\n",
        "      #  handleMixLabels Imputation\n",
        "      ('handleMixLabels Imputation rest_type', self.__handleMixLabels(\n",
        "          variables='rest_type', target=self.__target, limit=15)),   \n",
        "\n",
        "      #  handleMixLabels Imputation\n",
        "      ('handleMixLabels Imputation cuisines', self.__handleMixLabels(\n",
        "          variables='cuisines', target=self.__target, limit=15)),         \n",
        "\n",
        "      # Feature Transformation\n",
        "      ('LogTransformer',LogTransformer(\n",
        "          variables=self.__LogTransformerVarables)),\n",
        "\n",
        "      #  Ordinal Encoder\n",
        "      ('OrdinalEncoder',OrdinalEncoder(\n",
        "          encoding_method='ordered',variables=self.__OrdinalEncoderVariables,unseen='ignore')),\n",
        "\n",
        "      #  OneHotEncoder\n",
        "      ('OneHotEncoder',OneHotEncoder(\n",
        "          drop_last=True,variables=self.__OneHotEncoderVariables)),\n",
        "\n",
        "      # #  feature selection\n",
        "      # ('feature selection',SelectFromModel(\n",
        "      #     Lasso(alpha=self.__alpha, random_state=self.__random_state)))\n",
        "    ])\n",
        "\n",
        "    return pipe\n"
      ],
      "metadata": {
        "id": "PFjsyhbgZaCk"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/gdrive/MyDrive/zomato.csv')"
      ],
      "metadata": {
        "id": "FgC9AoxBNIQ2"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zmt = ZomatoModelTrain(df)"
      ],
      "metadata": {
        "id": "1FNr6WINyFua"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_test,y_train,y_test = zmt.applyModelTrain()"
      ],
      "metadata": {
        "id": "8H7utpO1ysl-"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train['rate'],X_test['rate'] = zmt.handleRating(X_train,X_test,'rate')"
      ],
      "metadata": {
        "id": "T2e-gEmgJ3Qd"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train['rate'].unique(),X_test['rate'].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEcfeX2pM0UX",
        "outputId": "11bf8acf-d8e1-4d1e-d8a0-154929d533f9"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array(['Good', 'Average', 'New', 'Poor'], dtype=object),\n",
              " array(['Average', 'New', 'Good', 'Poor'], dtype=object))"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_test = zmt.handleRanking(X_train,X_test,'listed_in(type)')"
      ],
      "metadata": {
        "id": "1Lbk0RafDMF7"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_test = zmt.handleRanking(X_train,X_test,'listed_in(city)')"
      ],
      "metadata": {
        "id": "ykkmfGefNwCQ"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_test = zmt.handleRanking(X_train,X_test,'location')"
      ],
      "metadata": {
        "id": "5xjOhIY3Oh5W"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = zmt.featurePipeline()"
      ],
      "metadata": {
        "id": "QaJUNNKL5ILw"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BurlxqdHKMOz",
        "outputId": "0d1416df-165d-4e93-8e30-72aacac25239"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('Add missing indicator',\n",
              "                 AddMissingIndicator(variables=['votes', 'rate', 'location'])),\n",
              "                ('Median Missing Imputation',\n",
              "                 MeanMedianImputer(variables=['votes', 'location'])),\n",
              "                ('Mode Missing Imputation',\n",
              "                 CategoricalImputer(imputation_method='frequent',\n",
              "                                    variables=['rate', 'cuisines',\n",
              "                                               'rest_type'])),\n",
              "                ('handleMixLabels Imputation rest_type',\n",
              "                 handleMixLabels(li...\n",
              "                 handleMixLabels(limit=15, target='approx_cost(for two people)',\n",
              "                                 variables='cuisines')),\n",
              "                ('LogTransformer', LogTransformer(variables=['votes'])),\n",
              "                ('OrdinalEncoder',\n",
              "                 OrdinalEncoder(variables=['rate', 'location',\n",
              "                                           'listed_in(type)',\n",
              "                                           'listed_in(city)'])),\n",
              "                ('OneHotEncoder',\n",
              "                 OneHotEncoder(drop_last=True,\n",
              "                               variables=['online_order', 'book_table',\n",
              "                                          'rest_type', 'cuisines']))])"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApeYz66MK5B0",
        "outputId": "61165ace-fcc6-40d6-a7ef-5b3662886fd7"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 34650 entries, 0 to 34649\n",
            "Data columns (total 9 columns):\n",
            " #   Column           Non-Null Count  Dtype  \n",
            "---  ------           --------------  -----  \n",
            " 0   online_order     34650 non-null  object \n",
            " 1   book_table       34650 non-null  object \n",
            " 2   rate             34650 non-null  object \n",
            " 3   votes            27944 non-null  float64\n",
            " 4   location         34650 non-null  int64  \n",
            " 5   rest_type        34514 non-null  object \n",
            " 6   cuisines         34621 non-null  object \n",
            " 7   listed_in(type)  34650 non-null  int64  \n",
            " 8   listed_in(city)  34650 non-null  int64  \n",
            "dtypes: float64(1), int64(3), object(5)\n",
            "memory usage: 2.4+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape,y_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cx3QUw-Czlw",
        "outputId": "d8fcb0fd-5f29-4378-8052-10f8c95f54e2"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((34650, 9), (34650, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipe.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "vqE0RQid5CmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe.transform"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWsRDL3s7S_N",
        "outputId": "4636d67d-e3e4-481a-ab4f-4e5597ca6a54"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function sklearn.pipeline.Pipeline.transform(self, X)>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import threading"
      ],
      "metadata": {
        "id": "zmz3trVN4UTk"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(threading.current_thread().is_alive())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWbxkMpe4X6Z",
        "outputId": "e3b4dc11-fc87-4c30-9c5a-2ef45f6540ac"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    }
  ]
}