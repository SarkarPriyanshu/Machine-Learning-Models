{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOxq5z6xwZtMVS8NpWA+zqs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SarkarPriyanshu/Machine-Learning-Models/blob/main/Zomato_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install feature_engine"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VY_lBjJjWyH",
        "outputId": "eb2831e5-4d34-4be3-cab1-d58f4e19e2e0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting feature_engine\n",
            "  Downloading feature_engine-1.5.2-py2.py3-none-any.whl (290 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.0/290.0 KB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from feature_engine) (1.0.2)\n",
            "Requirement already satisfied: statsmodels>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from feature_engine) (0.12.2)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from feature_engine) (1.7.3)\n",
            "Requirement already satisfied: pandas>=1.0.3 in /usr/local/lib/python3.8/dist-packages (from feature_engine) (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.18.2 in /usr/local/lib/python3.8/dist-packages (from feature_engine) (1.21.6)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.0.3->feature_engine) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.0.3->feature_engine) (2.8.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=1.0.0->feature_engine) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=1.0.0->feature_engine) (3.1.0)\n",
            "Requirement already satisfied: patsy>=0.5 in /usr/local/lib/python3.8/dist-packages (from statsmodels>=0.11.1->feature_engine) (0.5.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from patsy>=0.5->statsmodels>=0.11.1->feature_engine) (1.15.0)\n",
            "Installing collected packages: feature_engine\n",
            "Successfully installed feature_engine-1.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "56-lh5eoY19C"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "import re\n",
        "from typing import List, Optional, Union, Dict\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import scipy.stats as stats\n",
        " \n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn.pipeline import Pipeline\n",
        "from feature_engine.imputation import MeanMedianImputer,AddMissingIndicator,CategoricalImputer\n",
        "from feature_engine.transformation import LogTransformer\n",
        "from feature_engine.encoding import OrdinalEncoder,OneHotEncoder\n",
        "\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "# to build the models\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/gdrive/', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-Vp-ef-Y8fM",
        "outputId": "abb7abe7-b3b8-4bf6-ebbb-6ee1353b9d7c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class handleMixLabels(BaseEstimator, TransformerMixin):\n",
        "  def __init__(self,variables: Union[None, str, List[str]] = None,target:str=None,tol:Union[None, int, List[int]]=10):\n",
        "    self.variables = variables\n",
        "    self.keys:Union[None,List[Union[str, Dict[Union[str, int], List[Union[str, int]]]]]] = None\n",
        "    self.tol = tol\n",
        "    self.target = target\n",
        "    self.all_unique_feature_type:Union[None,List[Union[str, Dict[Union[str, int], List[Union[str, int]]]]]] = None\n",
        "    self.top_unique_feature_type_:Union[None,List[Union[str, Dict[Union[str, int], List[Union[str, int]]]]]] = None\n",
        "    self.unique_rare_feature_type_:Union[None,List[Union[str, Dict[Union[str, int], List[Union[str, int]]]]]] = None\n",
        "\n",
        "\n",
        "\n",
        "  # we check the top 10 cardinal value which have monotonic relation with target column store that in keys\n",
        "  # go through the column if it have mixed or comma seperated values the carefully store all unique cardinal values store in all_unique_feature_type\n",
        "  # based on tolerance it take top cardinal values default value is 10 and store those top values in top_unique_feature_type_\n",
        "  def fit(self, X: pd.DataFrame,y:pd.Series):\n",
        "        X = X.dropna().copy()\n",
        "        X[self.target] = y\n",
        "        X[self.target] = X[self.target].fillna(X[self.target].median())\n",
        "       \n",
        "        if X[self.target].dtypes != float:\n",
        "          X[self.target] = X[self.target].str.replace(',','').astype(float)\n",
        "\n",
        "        if isinstance(self.variables,list):\n",
        "          self.keys = list()\n",
        "          variable_keys = list()\n",
        "          self.all_unique_feature_type = list()\n",
        "          self.top_unique_feature_type_ = list()\n",
        "          self.unique_rare_feature_type_ = list()\n",
        "\n",
        "          for index in range(0,len(self.variables)):\n",
        "\n",
        "            # fetch all top categories of variable related to target variables in descending order \n",
        "            for key in X.groupby(self.variables[index])[self.target].mean().sort_values(ascending=False).keys():\n",
        "                    variable_keys.append(key)\n",
        "            self.keys.append({self.variables[index]:variable_keys})\n",
        "            variable_keys = list()\n",
        "\n",
        "            # remove duplicated categories as they are all mixed up\n",
        "            self.all_unique_feature_type.append({self.variables[index]:list()})\n",
        "           \n",
        "            # collect all unique categories from variables   \n",
        "            for value in X[self.variables[index]]:\n",
        "              if ',' in value:\n",
        "                for item in value.split(','):\n",
        "                  if item.strip() not in self.all_unique_feature_type[index][self.variables[index]]:\n",
        "                    self.all_unique_feature_type[index][self.variables[index]].append(item.strip())\n",
        "              else:\n",
        "                if value.strip() not in self.all_unique_feature_type[index][self.variables[index]]:\n",
        "                  self.all_unique_feature_type[index][self.variables[index]].append(value.strip())  \n",
        "            \n",
        "            self.top_unique_feature_type_.append({self.variables[index]:list()})\n",
        "\n",
        "            # Seperate less related unique categories from top categorials for rare labeling\n",
        "            for value in self.keys[index][self.variables[index]][:self.tol[index]]:\n",
        "              if isinstance(value,tuple):\n",
        "                for item in value:\n",
        "                  if item.strip() not in self.top_unique_feature_type_[index][self.variables[index]]:\n",
        "                      self.top_unique_feature_type_[index][self.variables[index]].append(item.strip())\n",
        "              elif ',' in value:\n",
        "                  for item in value.split(','):\n",
        "                    if item.strip() not in self.top_unique_feature_type_[index][self.variables[index]]:\n",
        "                      self.top_unique_feature_type_[index][self.variables[index]].append(item.strip())\n",
        "              else:\n",
        "                if value.strip() not in self.top_unique_feature_type_[index][self.variables[index]]:\n",
        "                  self.top_unique_feature_type_[index][self.variables[index]].append(value.strip())\n",
        "\n",
        "            self.top_unique_feature_type_[index][self.variables[index]] = self.top_unique_feature_type_[index][self.variables[index]] + ['Rare']\n",
        "            \n",
        "            self.unique_rare_feature_type_.append({self.variables[index]:list()})\n",
        "            self.unique_rare_feature_type_[index][self.variables[index]] = [value for value in self.all_unique_feature_type[index][self.variables[index]] if value not in self.top_unique_feature_type_[index][self.variables[index]]]\n",
        "\n",
        "            \n",
        "        if isinstance(self.variables,str):\n",
        "          self.keys = list()\n",
        "          self.all_unique_feature_type = list()\n",
        "          self.top_unique_feature_type_ = list()\n",
        "          self.unique_rare_feature_type_ = list()  \n",
        "\n",
        "          # fetch all top categories of variable related to target variables in descending order \n",
        "          for key in X.groupby(self.variables)[self.target].mean().sort_values(ascending=False).keys():\n",
        "                    self.keys.append(key)\n",
        "           \n",
        "          # collect all unique categories from variables   \n",
        "          for value in X[self.variables]:\n",
        "              if ',' in value:\n",
        "                for item in value.split(','):\n",
        "                  if item.strip() not in self.all_unique_feature_type:\n",
        "                    self.all_unique_feature_type.append(item.strip())\n",
        "              else:\n",
        "                if value.strip() not in self.all_unique_feature_type:\n",
        "                  self.all_unique_feature_type.append(value.strip())  \n",
        "            \n",
        "\n",
        "          # Seperate less related unique categories from top categorials for rare labeling\n",
        "          for value in self.keys[:self.tol]:\n",
        "              if isinstance(value,tuple):\n",
        "                for item in value:\n",
        "                  if item.strip() not in self.top_unique_feature_type_:\n",
        "                      self.top_unique_feature_type_.append(item.strip())\n",
        "              elif ',' in value:\n",
        "                  for item in value.split(','):\n",
        "                    if item.strip() not in self.top_unique_feature_type_:\n",
        "                      self.top_unique_feature_type_.append(item.strip())\n",
        "              else:\n",
        "                if value.strip() not in self.top_unique_feature_type_:\n",
        "                  self.top_unique_feature_type_.append(value.strip())\n",
        "\n",
        "          self.top_unique_feature_type_ = self.top_unique_feature_type_ + ['Rare']  \n",
        "          self.unique_rare_feature_type_ = [value for value in self.all_unique_feature_type if value not in self.top_unique_feature_type_]\n",
        "\n",
        "\n",
        "        return self\n",
        "\n",
        "\n",
        "\n",
        "  def transform(self, X: pd.DataFrame):\n",
        "        X = X.copy()\n",
        "        \n",
        "        if isinstance(self.variables,list):\n",
        "          for index in range(0,len(self.variables)):\n",
        "            # adding new columns of unique labels in datasets\n",
        "            for value in self.top_unique_feature_type_[index][self.variables[index]]:\n",
        "              X[f'{self.variables[index]}_{value}'] = np.zeros(X.shape[0])\n",
        "                    \n",
        "              X[f'{self.variables[index]}_{value}'].astype(int)   \n",
        "\n",
        "            # Adding 1 and 0's to those newly added columns\n",
        "            for value in self.top_unique_feature_type_[index][self.variables[index]]:\n",
        "              for indx in range(0,X.shape[0]):\n",
        "                if value in X[self.variables[index]][indx]:\n",
        "                  X[f'{self.variables[index]}_{value}'][indx] = 1\n",
        "                      \n",
        "            for value in self.unique_rare_feature_type_[index][self.variables[index]]:\n",
        "              for indx in range(0,X.shape[0]):\n",
        "                if value in X[self.variables[index]][indx]:\n",
        "                  X[f'{self.variables[index]}_Rare'][indx] = 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "          for index in range(0,len(self.variables)):\n",
        "            # adding new columns of unique labels in datasets\n",
        "            for value in self.top_unique_feature_type_[index][self.variables[index]]:\n",
        "              X[f'{self.variables[index]}_{value}'] = np.zeros(X.shape[0])\n",
        "\n",
        "            # Adding 1 and 0's to those newly added columns\n",
        "            for value in self.top_unique_feature_type_[index][self.variables[index]]:\n",
        "              for indx in range(0,X.shape[0]):\n",
        "                if  ',' in  X[self.variables[index]][indx]:\n",
        "                  for item in X[self.variables[index]][indx].split(','):\n",
        "                    if item.strip() not in self.unique_rare_feature_type_[index][self.variables[index]]:\n",
        "                        X[f'{self.variables[index]}_{value}'][indx] = 1\n",
        "                    if item.strip() in self.unique_rare_feature_type_[index][self.variables[index]]:\n",
        "                      X[f'{self.variables[index]}_Rare'][indx] = 1\n",
        "                else:\n",
        "                  if value.strip() == X[self.variables[index]][indx].strip():\n",
        "                        X[f'{self.variables[index]}_{value}'][indx] = 1\n",
        "                  if X[self.variables[index]][indx].strip() in self.unique_rare_feature_type_[index][self.variables[index]]:\n",
        "                      X[f'{self.variables[index]}_Rare'][indx] = 1\n",
        "\n",
        "        if isinstance(self.variables,str):  \n",
        "         # adding new columns of unique labels in datasets\n",
        "          for value in self.top_unique_feature_type_:\n",
        "            X[f'{self.variables}_{value}'] = np.zeros(X.shape[0])\n",
        "                  \n",
        "            X[f'{self.variables}_{value}'].astype(int)   \n",
        "\n",
        "          # Adding 1 and 0's to those newly added columns\n",
        "          for value in self.top_unique_feature_type_:\n",
        "            for index in range(0,X.shape[0]):\n",
        "              if value in X[self.variables][index]:\n",
        "                X[f'{self.variables}_{value}'][index] = 1\n",
        "                    \n",
        "          for value in self.unique_rare_feature_type_:\n",
        "            for index in range(0,X.shape[0]):\n",
        "              if value in X[self.variables][index]:\n",
        "                X[f'{self.variables}_Rare'][index] = 1\n",
        "       \n",
        "        X = X.drop(self.variables,axis=1)\n",
        "        return X"
      ],
      "metadata": {
        "id": "IwIE9MNidizS"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class handleRankingLabels(BaseEstimator, TransformerMixin):\n",
        "  def __init__(self,variables: Union[None, str, List[str]] = None,target:str=None,tol:Union[None, int, List[int]]=10):\n",
        "        self.variables = variables\n",
        "        self.listed_in_ranks:Union[None,List[Union[str, Dict[Union[str, int], List[Union[str, int]]]]]] = None\n",
        "        self.tol = tol\n",
        "        self.target = target\n",
        "        self.listed_in_dict:Union[None,Dict,List[Union[str, Dict[Union[str, int], List[Union[str, int]]]]]] = None\n",
        "\n",
        "        if (isinstance(self.variables,list) and isinstance(self.tol,list)) and (len(self.variables) != len(self.tol)):\n",
        "          raise Exception(\"Number of variable and number of tolerance should be equal in length, check the varables and tol!!\")\n",
        "\n",
        "  \n",
        "  # This method checks relation with target columns based on that montonic relation assign ordinal values to top 10 features and map those feature on train and test datasets \n",
        "  def fit(self,X:pd.DataFrame,y:pd.Series = None):\n",
        "    X[self.target] = y\n",
        "    X[self.target] = X[self.target].fillna(X[self.target].median())\n",
        "       \n",
        "    if X[self.target].dtypes != float:\n",
        "      X[self.target] = X[self.target].str.replace(',','').astype(float) \n",
        "\n",
        "    if isinstance(self.variables,list):\n",
        "      self.listed_in_ranks = list()\n",
        "      for index in range(0,len(self.variables)):\n",
        "        if len(df[self.variables[index]].unique()) > 10:\n",
        "          # Get unique top 10 appering categories\n",
        "          self.listed_in_ranks.append({f'{self.variables[index]}':list(X.groupby(self.variables[index])[self.target].mean().sort_values(ascending=False)[:self.tol[index]].to_dict().keys())})\n",
        "        else:\n",
        "          # Get unique top 10 appering categories\n",
        "          self.listed_in_ranks.append({f'{self.variables[index]}':list(X.groupby(self.variables[index])[self.target].mean().sort_values(ascending=False).to_dict().keys())})\n",
        "    \n",
        "    if isinstance(self.variables,str):\n",
        "      self.listed_in_ranks = list()\n",
        "      if len(df[self.variables].unique()) > 10:\n",
        "          # Get unique top 10 appering categories\n",
        "          self.listed_in_ranks = list(X.groupby(self.variables)[self.target].mean().sort_values(ascending=False)[:self.tol].to_dict().keys())\n",
        "      else:\n",
        "          # Get unique top 10 appering categories\n",
        "          self.listed_in_ranks = list(X.groupby(self.variables)[self.target].mean().sort_values(ascending=False).to_dict().keys())   \n",
        "\n",
        "    return self\n",
        "\n",
        "  def transform(self,X):\n",
        "\n",
        "    if isinstance(self.variables,list):\n",
        "      self.listed_in_dict = list()\n",
        "      list_in_dict = dict()\n",
        "      for index in range(0,len(self.variables)):\n",
        "        # replacing non top categories  \n",
        "        X[self.variables[index]] = X[self.variables[index]].apply(lambda value:value if value in self.listed_in_ranks[index][self.variables[index]] else 'Rare')   \n",
        "              \n",
        "        self.listed_in_ranks[index][self.variables[index]] = self.listed_in_ranks[index][self.variables[index]]+ ['Rare']\n",
        "        # # Creating dictionary for mapping categories \n",
        "        for indx in range(0,len(self.listed_in_ranks[index][self.variables[index]])):\n",
        "           list_in_dict[self.listed_in_ranks[index][self.variables[index]][indx]] = indx\n",
        "        self.listed_in_dict.append({self.variables[index]:list_in_dict})\n",
        "\n",
        "        list_in_dict = dict()\n",
        "        # # replacing categories\n",
        "        X[self.variables[index]] = X[self.variables[index]].map(self.listed_in_dict[index][self.variables[index]]) \n",
        "    \n",
        "    if isinstance(self.variables,str):\n",
        "      self.listed_in_dict = dict()\n",
        "      # replacing non top categories  \n",
        "      X[self.variables] = X[self.variables].apply(lambda value:value if value in self.listed_in_ranks else 'Rare')   \n",
        "            \n",
        "      self.listed_in_ranks = self.listed_in_ranks + ['Rare']\n",
        "      # # Creating dictionary for mapping categories \n",
        "      for index in range(0,len(self.listed_in_ranks)):\n",
        "        self.listed_in_dict[self.listed_in_ranks[index]] = index\n",
        "\n",
        "      # # replacing categories\n",
        "      X[self.variables] = X[self.variables].map(self.listed_in_dict) \n",
        "\n",
        "    return X"
      ],
      "metadata": {
        "id": "xFWdtwFKJ7oB"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class featureSelection(BaseEstimator, TransformerMixin):\n",
        "  def __init__(self,alpha,random_state):\n",
        "    self.selected_feats = None\n",
        "    self.__alpha = alpha\n",
        "    self.__random_state = random_state \n",
        "    self.sel_ = None\n",
        "\n",
        "  def fit(self,X,y):\n",
        "   self.sel_ = SelectFromModel(Lasso(alpha=self.__alpha, random_state=self.__random_state))\n",
        "   self.sel_.fit(X, y)  \n",
        "   return self\n",
        "\n",
        "  def transform(self,X): \n",
        "   self.selected_feats = X_train.columns[(self.sel_.get_support())]\n",
        "   return X[self.selected_feats]"
      ],
      "metadata": {
        "id": "ohBNq1znssgZ"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ZomatoModelTrain(handleMixLabels,handleRankingLabels,featureSelection):\n",
        "\n",
        "  def __init__(self,df):\n",
        "    self.__df = df\n",
        "    self.__target = 'approx_cost(for two people)'\n",
        "    self.__df[self.__target] = self.__df[self.__target].str.replace(',','').astype(float)\n",
        "    self.__random_state = 100\n",
        "    self.__alpha = 0.001\n",
        "    self.__test_size = 0.33\n",
        "    self.__handleRankingLabels = handleRankingLabels\n",
        "    self.__handleMixLabels = handleMixLabels\n",
        "    self.__featureSelection = featureSelection\n",
        "    self.__variable_to_drop = ['url','address','phone','reviews_list','name','dish_liked','menu_item'] + [self.__target]\n",
        "    self.__AddMissingIndicatorVariables = ['votes','rate']\n",
        "    self.__MeanMedianImputerVarables = ['votes']\n",
        "    self.__CategoricalImputerModeVarables = ['rate','cuisines','rest_type']\n",
        "    self.__LogTransformerVarables = ['votes']\n",
        "    self.__OrdinalEncoderVariables = ['rate']\n",
        "    self.__OneHotEncoderVariables = ['online_order','book_table']\n",
        "    self.__handleRankingLabels_var = {'variables':['listed_in(type)','listed_in(city)','location'],'tolerance':[10,15,15]}\n",
        "    self.__handleMixLabels_var ={'variables':['rest_type','cuisines'],'tolerance':[10,15]}\n",
        "\n",
        "\n",
        "  def applyModelTrain(self):\n",
        "    X_train,X_test,y_train,y_test = self.__dataSpliter()\n",
        "    X_train,X_test = self.__dataCleanar(X_train,X_test)  \n",
        "    return X_train,X_test,y_train.fillna(y_train.median()),y_test.fillna(y_test.median())\n",
        "\n",
        "  # Splits the data into train and test set\n",
        "  def __dataSpliter(self):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(df.drop(self.__variable_to_drop,axis=1), df[self.__target], test_size=self.__test_size, random_state=42)\n",
        "    X_train,X_test,y_train,y_test = X_train.reset_index().drop('index',axis=1),X_test.reset_index().drop('index',axis=1),y_train.reset_index().drop('index',axis=1),y_test.reset_index().drop('index',axis=1)\n",
        "    X_train,X_test = self.__dataCleanar(X_train,X_test)\n",
        "    return X_train,X_test,y_train,y_test\n",
        "\n",
        "  # This method andle noisy data from rate and votes columns\n",
        "  def __dataCleanar(self,X_train,X_test):\n",
        "    # replacing '-' with nan in rate variable\n",
        "    X_train['rate'] = X_train['rate'].replace('-',np.nan)  \n",
        "    X_test['rate'] = X_test['rate'].replace('-',np.nan)\n",
        "\n",
        "    # replacing '0' with nan in votes variable\n",
        "    X_train['votes'] = X_train['votes'].replace(0,np.nan)\n",
        "    X_test['votes'] = X_test['votes'].replace(0,np.nan)\n",
        "    \n",
        "    # replacing '/5' with ' in rates variable\n",
        "    X_train['rate'] = X_train['rate'].apply(lambda value:str(value).replace('/5',''))\n",
        "    X_test['rate'] = X_test['rate'].apply(lambda value:str(value).replace('/5',''))\n",
        "    \n",
        "    X_train['rate'] = X_train['rate'].apply(lambda value: 0 if value == 'NEW' else value).astype(float)\n",
        "    X_test['rate'] = X_test['rate'].apply(lambda value: 0 if value == 'NEW' else value).astype(float)\n",
        "\n",
        "    return X_train,X_test\n",
        "\n",
        "  # This method is for rate column where based on certain ranges we made this column to categorical oridinal variable\n",
        "  def handleRating(self,X_train,X_test,feature):\n",
        "    return np.where(X_train[feature]==np.nan,np.nan,np.where(X_train[feature]==0,'New',np.where(X_train[feature]<2.5,'Poor',np.where((X_train[feature]>2.5) | (X_train[feature]<3.5),'Average','Good')))),np.where(X_test[feature]==np.nan,np.nan,np.where(X_test[feature]==0,'New',np.where(X_test[feature]<2.5,'Poor',np.where((X_test[feature]>2.5) | (X_test[feature]<3.5),'Average','Good')))) \n",
        "\n",
        "  def featurePipeline(self):\n",
        "    pipe = Pipeline([\n",
        "      #  Missing indicator\n",
        "      ('Add missing indicator',AddMissingIndicator(\n",
        "          variables=self.__AddMissingIndicatorVariables)),\n",
        "    \n",
        "      #   Median Missing Imputation\n",
        "      ('Median Missing Imputation',MeanMedianImputer(\n",
        "          imputation_method='median', variables=self.__MeanMedianImputerVarables)),\n",
        "\n",
        "      #   Mode Missing Imputation\n",
        "      ('Mode Missing Imputation',CategoricalImputer(\n",
        "          imputation_method='frequent', variables=self.__CategoricalImputerModeVarables)),        \n",
        "\n",
        "      # Feature Transformation\n",
        "      ('LogTransformer',LogTransformer(\n",
        "          variables=self.__LogTransformerVarables)),\n",
        "\n",
        "      #  Ordinal Encoder\n",
        "      ('OrdinalEncoder',OrdinalEncoder(\n",
        "          encoding_method='ordered',variables=self.__OrdinalEncoderVariables)),\n",
        "\n",
        "      #  OneHotEncoder\n",
        "      ('OneHotEncoder',OneHotEncoder(\n",
        "          drop_last=True,variables=self.__OneHotEncoderVariables)),\n",
        "\n",
        "      # handleRankingLabels\n",
        "      ('handleRankingLabels listed_in(type)',self.__handleRankingLabels(\n",
        "          variables=self.__handleRankingLabels_var['variables'],tol=self.__handleRankingLabels_var['tolerance'],target=self.__target)),        \n",
        "\n",
        "      #  handleMixLabels Imputation\n",
        "      ('handleMixLabels Imputation cuisines', self.__handleMixLabels(\n",
        "          variables=self.__handleMixLabels_var['variables'],tol=self.__handleMixLabels_var['tolerance'],target=self.__target)), \n",
        "\n",
        "      #  feature selection\n",
        "      ('feature selection',featureSelection(alpha=self.__alpha, random_state=self.__random_state)),\n",
        "\n",
        "      # # Model\n",
        "      # ('Random Forest Model',RandomForestRegressor(bootstrap= True,max_depth= 10,min_samples_leaf= 2,min_samples_split= 2,n_estimators= 100,oob_score= True))\n",
        "    ])\n",
        "\n",
        "    return pipe\n"
      ],
      "metadata": {
        "id": "PFjsyhbgZaCk"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/gdrive/MyDrive/zomato.csv')"
      ],
      "metadata": {
        "id": "FgC9AoxBNIQ2"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zmt = ZomatoModelTrain(df)"
      ],
      "metadata": {
        "id": "1FNr6WINyFua"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_test,y_train,y_test = zmt.applyModelTrain()"
      ],
      "metadata": {
        "id": "8H7utpO1ysl-"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xVu433aTFa5",
        "outputId": "fc4ecd9a-a42a-463f-bfc5-4d8e92762dab"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 34650 entries, 0 to 34649\n",
            "Data columns (total 9 columns):\n",
            " #   Column           Non-Null Count  Dtype  \n",
            "---  ------           --------------  -----  \n",
            " 0   online_order     34650 non-null  object \n",
            " 1   book_table       34650 non-null  object \n",
            " 2   rate             29422 non-null  float64\n",
            " 3   votes            27944 non-null  float64\n",
            " 4   location         34638 non-null  object \n",
            " 5   rest_type        34514 non-null  object \n",
            " 6   cuisines         34621 non-null  object \n",
            " 7   listed_in(type)  34650 non-null  object \n",
            " 8   listed_in(city)  34650 non-null  object \n",
            "dtypes: float64(2), object(7)\n",
            "memory usage: 2.4+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train['rate'],X_test['rate'] = zmt.handleRating(X_train,X_test,'rate')"
      ],
      "metadata": {
        "id": "T2e-gEmgJ3Qd"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train['rate'].unique(),X_test['rate'].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEcfeX2pM0UX",
        "outputId": "3f62552a-d94f-447a-9c9f-d6c46a926208"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array(['Good', 'Average', 'New', 'Poor'], dtype=object),\n",
              " array(['Average', 'New', 'Good', 'Poor'], dtype=object))"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = zmt.featurePipeline()"
      ],
      "metadata": {
        "id": "pJZMIasqRqct"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipe.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "p8dMOIakFORx"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = pipe.transform(X_train)"
      ],
      "metadata": {
        "id": "05BaFOMOwz_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.sample(5)"
      ],
      "metadata": {
        "id": "kCpJYn_CVtcu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}